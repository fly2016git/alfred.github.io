### 面试要点

#### 算法和工具

1. RNN

![img](https://pic4.zhimg.com/80/v2-f716c816d46792b867a6815c278f11cb_720w.jpg)

这里：

![[公式]](https://www.zhihu.com/equation?tex=x) 为当前状态下数据的输入， ![[公式]](https://www.zhihu.com/equation?tex=h) 表示接收到的上一个节点的输入。

![[公式]](https://www.zhihu.com/equation?tex=y) 为当前节点状态下的输出，而 ![[公式]](https://www.zhihu.com/equation?tex=h%27) 为传递到下一个节点的输出。

通过上图的公式可以看到，输出 **h'** 与 **x** 和 **h** 的值都相关。

而 **y** 则常常使用 **h'** 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。

对这里的**y**如何通过 **h'** 计算得到往往看具体模型的使用方式。

通过序列形式的输入，我们能够得到如下形式的RNN。

![img](https://pic2.zhimg.com/80/v2-71652d6a1eee9def631c18ea5e3c7605_720w.jpg)

整体的结构：

![标准 RNN 中的重复模块包含单一的层](https://img-blog.csdnimg.cn/20181107085032376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hhcHB5Um9ja2luZw==,size_16,color_FFFFFF,t_70)





2. LSTM

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

LSTM结构（图右）和普通RNN的主要输入输出区别如下所示：

![img](https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_720w.jpg)

相比RNN只有一个传递状态 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et+) ，LSTM有两个传输状态，一个 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) （cell state），和一个 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) （hidden state）。（Tips：RNN中的 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 对于LSTM中的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) ）

其中对于传递下去的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) 改变得很慢，通常输出的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) 是上一个状态传过来的 ![[公式]](https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D) 加上一些数值。而 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 则在不同节点下往往会有很大的区别。

###### LSTM结构剖析：

首先使用LSTM的当前输入 ![[公式]](https://www.zhihu.com/equation?tex=x%5Et) 和上一个状态传递下来的 ![[公式]](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 拼接训练得到四个状态：

![img](https://pic4.zhimg.com/80/v2-15c5eb554f843ec492579c6d87e1497b_720w.jpg)

![img](https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_720w.jpg)

其中， ![[公式]](https://www.zhihu.com/equation?tex=z%5Ef+) ， ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) ，![[公式]](https://www.zhihu.com/equation?tex=z%5Eo) 是由拼接向量乘以权重矩阵之后，再通过一个 ![[公式]](https://www.zhihu.com/equation?tex=sigmoid+) 激活函数转换成0到1之间的数值，来作为一种门控状态。而 ![[公式]](https://www.zhihu.com/equation?tex=z) 则是将结果通过一个 ![[公式]](https://www.zhihu.com/equation?tex=tanh) 激活函数将转换成-1到1之间的值（这里使用 ![[公式]](https://www.zhihu.com/equation?tex=tanh) 是因为这里是将其做为输入数据，而不是门控信号)。

**下面开始进一步介绍这四个状态在LSTM内部的使用。**

![img](https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_720w.jpg)

![[公式]](https://www.zhihu.com/equation?tex=%5Codot) 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 ![[公式]](https://www.zhihu.com/equation?tex=%5Coplus) 则代表进行矩阵加法。

LSTM内部主要有三个阶段：

\1. 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行**选择性**忘记。简单来说就是会 “忘记不重要的，记住重要的”。

具体来说是通过计算得到的 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ef) （f表示forget）来作为忘记门控，来控制上一个状态的 ![[公式]](https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D) 哪些需要留哪些需要忘。

\2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 ![[公式]](https://www.zhihu.com/equation?tex=x%5Et) 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 ![[公式]](https://www.zhihu.com/equation?tex=z+) 表示。而选择的门控信号则是由 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) （i代表information）来进行控制。

> 将上面两步得到的结果相加，即可得到传输给下一个状态的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) 。也就是上图中的第一个公式。

\3. 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 ![[公式]](https://www.zhihu.com/equation?tex=z%5Eo) 来进行控制的。并且还对上一阶段得到的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Eo) 进行了放缩（通过一个tanh激活函数进行变化）。

与普通RNN类似，输出 ![[公式]](https://www.zhihu.com/equation?tex=y%5Et) 往往最终也是通过 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 变化得到。

整体的结构：

![LSTM 中的重复模块包含四个交互的层](https://img-blog.csdnimg.cn/20181107085145510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hhcHB5Um9ja2luZw==,size_16,color_FFFFFF,t_70)



3. Attention

参考文献： [图解Attention](https://zhuanlan.zhihu.com/p/55614660) 

4. ESIM

模型结构：

![esim](.\图片\esim.png)

参考文献： [esim](https://zhuanlan.zhihu.com/p/73408108)



bert预训练模型（bert_base_chinese）为768维，所有的双向LSTM的隐层节点数为768，dropout rate为0.5，学习率为0.0004，采用adam优化器，第一动量设为0.9，第二动量设为0.999，batch size大小为32



###### 优化算法:

![image-20220515180938556](.\图片\image-20220515180938556.png)

![image-20220515181044877](.\图片\image-20220515181044877.png)



![image-20220515181245354](.\图片\image-20220515181245354.png)



![image-20220515181433261](.\图片\image-20220515181433261.png)

![image-20220515181835809](.\图片\image-20220515181835809.png)

![image-20220515182214813](.\图片\image-20220515182214813.png)

![image-20220515182428185](.\图片\image-20220515182428185.png)

Adam对学习率不敏感

###### 批量归一化

![image-20220515184559168](.\图片\image-20220515184559168.png)

![image-20220515184749586](.\图片\image-20220515184749586.png)

![image-20220515205300855](.\图片\image-20220515205300855.png)

![image-20220515205414918](.\图片\image-20220515205414918.png)



###### 过拟合和欠拟合

影响模型容量的两个因素：模型的参数，参数的取值范围

![image-20220515150959104](.\图片\image-20220515150959104.png)

###### l2正则：

![image-20220515151814262](.\图片\image-20220515151814262.png)





	5. XGboost



6. CRF



7. transformer



8. Earley



9. ES





10. Kafka



11. GB28181



12. TF-IDF



13. BERT



12. Dropout

![image-20220515154404983](.\图片\image-20220515154404983.png)

![image-20220515154605377](.\图片\image-20220515154605377.png)

![image-20220515154736771](.\图片\image-20220515154736771.png)



###### 权重初始化策略

![image-20220515163519291](.\图片\image-20220515163519291.png)

![image-20220515164220827](.\图片\image-20220515164220827.png)

![image-20220515164834644](.\图片\image-20220515164834644.png)



12. neo4j



14. Milvus 

距离计算： 欧氏距离

| Milvus 支持的索引                                            | 索引分类       | 适用场景                                                     |
| :----------------------------------------------------------- | :------------- | ------------------------------------------------------------ |
| [FLAT](https://milvus.io/cn/docs/v2.0.x/index.md#FLAT)       | N/A            | - 查询数据规模小   <br>- 需要 100% 的召回率<br>- 存储原始向量文件，每个都要去对比 |
| [IVF_FLAT](https://milvus.io/cn/docs/v2.0.x/index.md#IVF_FLAT) | 基于量化的索引 | - 高速查询<br>- 要求高召回率<br>- IVF_FLAT 它通过聚类方法把空间里的点划分至 `nlist` 个单元，然后比较目标向量与所有单元中心的距离，选出 `nprobe` 个最近单元。然后比较这些被选中单元里的所有向量，得到最终的结果，极大地缩短了查询时间。<br>- 由于 IVF_FLAT 未对原始的向量数据做任何压缩，IVF_FLAT 索引文件的大小与原始数据文件大小相当 |
| [IVF_SQ8](https://milvus.io/cn/docs/v2.0.x/index.md#IVF_SQ8) | 基于量化的索引 | - 高速查询 <br>- 磁盘和内存资源有限<br>- 查询召回率低于 IVF_FLAT<br>- 在IVF_FLAT的基础上，对向量进行标量量化（scalar quantization），能把原始向量中每个FLOAT（4 字节）转为UINT8（1 字节），从而可以把磁盘及内存、显存资源的消耗量减少为原来的 1/4 至 1/3 |
| [IVF_PQ](https://milvus.io/cn/docs/v2.0.x/index.md#IVF_PQ)   | 基于量化的索引 | - 超高速查询<br/>- 磁盘和内存资源有限<br/>- 可以接受偏低的查询召回率<br>- `PQ`（Product Quantization，乘积量化）会将原来的高维向量空间均匀分解成 `m` 个低维向量空间的笛卡尔积，然后对分解得到的低维向量空间分别做矢量量化。最终每条向量会存储在 `m` × `nbits` 个 bit 位里。乘积量化能将全样本的距离计算转化为到各低维空间聚类中心的距离计算，从而大大降低算法的时间复杂度 |
| [HNSW](https://milvus.io/cn/docs/v2.0.x/index.md#HNSW)       | 基于图的索引   | - 高速查询<br/>- 要求尽可能高的召回率<br/>- 内存空间大       |

###### `PQ`（Product Quantization，乘积量化）原理：

参考文献： http://www.javashuo.com/article/p-wmlecgbj-nq.html 

![img](https://pic2.zhimg.com/80/v2-3652caf4738a646ab0ba18b3a7ce573d_720w.jpg)

- 在训练阶段存在N个训练样本，样本维度是128，将128个维度切分成4个空间，则每个子空间维度为128/4。

- 分别对各个子向量聚类，每个子空间聚成256类，这样训练样本的每个子空间都可以用一个id表示，对应编码为类中心的id，这样训练样本就用一个很短的向量表示，到达量化的目的。

- 查询向量

- - 对称查询：其中x,y为原始未经压缩的向量，q(x),q(y)经过压缩之后的向量。计算x，y由q(x)和q(y)代替,因为q(x)和q(y)是已知的，所以可以提前算好，来加快检索。
  - 非对称查询：x和y之间的距离，由x和q(y)来代替，每个子空间的x与聚类中心计算距离，然后所有的子空间计算完成后，对同一个向量下各个子空间对不同聚类中心的距离求和，然后排序

###### Milvus常见概念

- Collection: 包含一组 entity，可以等价于关系型数据库系统（RDBMS）中的表
- Entity: 包含一组 field。field 与实际对象相对应。field 可以是代表对象属性的结构化数据，也可以是代表对象特征的向量。primary key 是用于指代一个 entity 的唯一值
- Partition: Milvus 支持将收集数据划分为物理存储上的多个部分。这个过程称为分区，每个分区可以包含多个段
- segment: Milvus 在数据插入时通过合并数据自动创建的数据文件。一个 collection 可以包含多个 segment。一个 segment 可以包含多个 entity。在搜索中，Milvus 会搜索每个 segment，并返回合并后的结果
- Shard 是指将数据写入操作分散到不同节点上，使 Milvus 能充分利用集群的并行计算能力进行写入。默认情况下单个 collection 包含 2 个分片（shard）。目前 Milvus 采用基于主键哈希的分片方式，未来将支持随机分片、自定义分片等更加灵活的分片方式。Partition 的意义在于通过划定分区减少数据读取，而shard 的意义在于多台机器上并行写入操作

#### 项目

#### 1. 数字视网膜智慧影像超融合系统

##### 总体流程

![搜索流程](.\图片\搜索-总体流程.png)



##### 消息队列

​	使用kafka作为中间消息队列, 主要支持两点.

- 高吞吐(可支持500Mb/s的数据吞吐量)

- 可缓存短期队列数据, 可供错误处理 / 冲刷数据 / 流量分析等使用场景

##### 数据存储

- 在数据存储部分, 我们采取结构化数据和向量数据分开存放的方式. 一方面减少整体数据存储量.

- 另一方面整合使用ES 和 Milvus两个数据库, 并组织数据访问接口. 对外实现一个支持向量搜索的NoSQL数据库.

##### 搜索功能

​	在向量+结构化的搜索时, 为了提高性能. 采用 Milvus + Redis的初筛方式, 遗漏的数据再通过Elasticsearch补充。

- 结构化搜索

​	结构化搜索, 是对结构化信息的搜索。采用NoSQL 数据库 Elasticsearch, 存储所有数据。

- 向量搜索

​	Milvus中的collection结构，以日期作为collection，地区分类左右partition,存储数据时以featureID (特征的唯一表示)作为Key, value 是特征向量。

- 向量 + 结构化搜索

​	通过Milvus完成向量搜索后, 可直接在Elasticsearch 集群中直接过滤属性

#### 2. 数字视网膜运维管理平台

![总体架构图](.\图片\运维平台总体架构.png)

![运维平台1](.\图片\运维平台2.png)



![运维平台3](.\图片\运维平台3.png)



#### 3. 宏观经济指标识别(NER)

项目描述：改进金融相关的问题中涉及到的宏观指标的识别和实体的规范化部分的实现。

- 数据说明：宏观经济指标库：通常是由冒号分割的几个词组成；人工标注的8000条宏观经济指标对应的不规范表达形式；人工标注的20000条宏观指标对应的pattern和核心词位置，pattern中包含了宏观指标中冒号分割的几个词对应的自定义类型；基于规则的版本得到的反馈数据筛选出10000条，主要是宏观指标的不规范表达数据；

- 数据处理：对宏观指标和标注pattern进行切分，按照metric-subword，subword-sublabel，subword-synonym的关联关系构建知识图谱，保存到neo4j；利用知识图谱，对宏观指标进行扩展，主要是将dist和view两个label对应的subword替换，生成新的指标，保存到es中；将subword和label进行2-gram切分，保存到es中；整理出正负样本，将标注数据中宏观经济指标及其对应的不规范表达形式一一对应，作为正样本；随机抽取8000个宏观指标，从宏观指标库中搜索与每个被选中指标相似的5个指标，作为负样本；

-  核心词识别：识别构成宏观指标的subword中那个是该指标的核心词，首先构建特征，包括当前subword的2-gram特征，其他subword的2-gram特征，当前指标中subword的数量，当前subword在宏观指标中的位置，当前指标中是否包含dist和view以及dist的类型；将上述特征输入到xgboost分类器，预测当前subword是否为核心词；

- 训练ESIM模型：利用公开的BERT预训练模型，生成宏观指标及其不规范表达的词嵌入表示，输入到ESIM模型进行训练；Early Stopping：验证集上的误差小于前t次epoch时得到的最优误差；

- 宏观指标识别：将问题进行2-gram切分，搜索es中的subword索引，对搜索到的subword根据规则进行过滤，得到候选subword集合；根据subword搜索es中的宏观指标索引，利用BM25评分算法进行评分，召回top50（评分相同的全部返回）；根据dist和核心词过滤搜索结果；截取宏观指标对应的原问题字符串，将该字符串分别与搜索结果配对，输入到ESIM，计算出相似度，选择相似度最高的结果作为识别结果。

#### 4. 智能问答系统

