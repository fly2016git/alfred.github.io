> 本文是对王树森教授强化学习课程的学习总结，原视频和课件地址： 
> 
> 视频地址：[https://youtu.be/vmkRMvhCW5c](https://youtu.be/vmkRMvhCW5c) 
> 
> 课件地址：[https://github.com/wangshusen/DeepLearning](https://github.com/wangshusen/DeepLearning) 

1. 智能体(agent)     

决策者或做动作的人，例如超级玛丽中的马里奥。

2. 环境(environment)  

环境是与agent交互的对象，可以抽象地理解为交互过程中的规则或机理。在超级玛丽游戏中，环境就是游戏程序。

3. 状态(state)  

每一时刻，都有一个状态，可以理解为对当前时刻环境的概括。在超级马里奥游戏中，可以把当前屏幕的画面(或最近几帧画面)当做状态。一般用s表示。

4. 动作(action)

agent基于当前环境状态所做的决策。例如马里奥向左、向右、向上的动作。一般用过a表示。

5. 状态空间 

所有可能存在的状态集合。可以是离散的，也可以是连续的；可能是有限的，也可能是无限的。  

状态转移：智能体从当前 t 时刻的状态 s 转移到下一个时刻状态为 s′的过程。状态转移通常是随机的，其随机性来自于环境。使用*状态转移概率函数*来描述状态转移，记作：  
$$ p_t(s'|s, a) = P(S_{t+1}^{'}=s'|S_t=s, A_t=a) $$
表示这个事件的概率：在当前状态$s$，智能体执行动作$a$，环境的状态变成$s'$


6. 奖励(reward)

指在智能体执行一个动作之后，环境返回给智能体的一个数值。奖励定义得好坏对强化学习的结果有非常大的影响。 

通常奖励是当前状态s、当前动作a、下一时刻状态 s′的函数，记作: $r(s, a, s')$

通常假设奖励函数是有界的，即对于所有的$a ∈ A$ 和$s,s'∈S$，有$|r(s, a, s′)| < \infty$   


7. 策略(policy)


9. 回报(return)与折扣回报


8. 价值函数


9. 随机性的来源


10. agent与环境的交互过程


